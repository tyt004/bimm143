---
title: "Class08"
author: "Tiffany"
date: "10/24/2019"
output: github_document
---

```{r}
# Generate some example data for clustering
tmp <- c(rnorm(30,-3), rnorm(30,3)) #takes clusters of 30 near the a value
x <- cbind(x=tmp, y=rev(tmp)) #binds the two clusters
plot(x)

```



Use the kmeans() function setting k to 2 and nstart=20
Inspect/print the results
Q. How many points are in each cluster? 
   30
Q. What ‘component’ of your result object details
 - cluster size?
 - cluster assignment/membership?
 - cluster center?
Plot x colored by the kmeans cluster assig



```{r}
k<-kmeans(x, centers = 2, nstart=20)
```


print k
```{r}
k
```


```{r}
k$size #to access something from a list use the dollar sign
```

```{r}
k$cluster# this is assignment/ membership
```


```{r}
k$centers #the access to the centers
```




```{r}
plot(x, col=k$cluster) #plots the clusters with two colors
points(k$centers, col="blue", pch=15) #highligts the center points of the two cluster
```


#hierarchecal cluster in R
The `hclust()` fucntion requres a distance matrix as input. 
You can get this from the `dist()` function

```{r}
# First we need to calculate point (dis)similarity
# as the Euclidean distance between observations
dist_matrix <- dist(x)
# The hclust() function returns a hierarchical
# clustering model
hc <- hclust(d = dist_matrix)
# the print method is not so useful here
hc 
```

```{r}
plot(hc)
abline(h=6, col="red") #whill show a red line on the dendogram where you want to cut
cutree(hc, h=6) # Cut by height h
#the results shows which cluster the data point belongs to (1 or 2)
```




# What do you notice?
The left side is all the >= 30 and right side is all above 30
# Does the dendrogram make sense based on your knowledge of x?
Yes, the smaller branches are the values very close to each other and they the next branch is the next closest dot to the pair and this cont.


```{r}
cutree(hc, k=2) #cuts into k groups
```



YOUR TURN
```{r}
# Step 1. Generate some example data for clustering
x <- rbind(
 matrix(rnorm(100, mean=0, sd = 0.3), ncol = 2), # c1
 matrix(rnorm(100, mean = 1, sd = 0.3), ncol = 2), # c2
 matrix(c(rnorm(50, mean = 1, sd = 0.3), # c3
 rnorm(50, mean = 0, sd = 0.3)), ncol = 2))
colnames(x) <- c("x", "y")
# Step 2. Plot the data without clustering
plot(x)
# Step 3. Generate colors for known clusters
# (just so we can compare to hclust results)
col <- as.factor( rep(c("c1","c2","c3"), each=50) )
plot(x, col=col)
```


```{r}
#clustering
dist_matrix2<- dist(x)
hc2 <- hclust(d = dist_matrix2)
#plot
plot(hc2)
abline(h=2, col="red") 
#cut tree into cluster or groups
grps<-cutree(hc2, k=3)
grps
```

plotthe data with color = to clusters

```{r}
plot(x, col=grps)
```

How many points in each cluster?
```{r}
table(grps)
```

Cross tabulate ie compare our clusterign result with the known answer
```{r}
table (grps,col)
```

#Principle component analysis (PCA)

```{r}
mydata <- read.csv("https://tinyurl.com/expression-CSV",
 row.names=1) 
head(mydata)
```


How many genes are in this data sheet?
```{r}
dim(mydata)
nrow(mydata)
```

Lets do PCA
```{r}
## lets do PCA
pca<- prcomp(t(mydata), scale=TRUE) #t transposes the data

## See what is returned by the prcomp() function
attributes(pca) 
```


```{r}eval= FALSE
## lets do PCA
pca <- prcomp(t(mydata), scale=TRUE)
## A basic PC1 vs PC2 2-D plot
plot(pca$x[,1], pca$x[,2])
## Precent variance is often more informative to look at
pca.var <- pca$sdev^2
pca.var.per <- round(pca.var/sum(pca.var)*100, 1)

pca.var.per
 [1] 91.0 2.8 
```


```{r}
pca.var <- pca$sdev^2
pca.var.per <- round(pca.var/sum(pca.var)*100, 1)

barplot(pca.var.per, main="Scree Plot",
 xlab="Principal Component", ylab="Percent Variation")
```

```{r}
#plot(pca$x,[1],pca$x,[2])
#blue, blue, blue, blue
```





```{r}
x<-read.csv("UK_foods.csv", row.names = 1)
dim(x)
```

Preview the first 6 rows
```{r}
head(x)
```



```{r}
barplot(as.matrix(x), beside=T, col=rainbow(nrow(x)))
pairs(x, col=rainbow(10), pch=16)
```



```{r}
# Use the prcomp() PCA function 
pca <- prcomp( t(x) )
summary(pca)
```


```{r}
plot(pca$x[,1], pca$x[,2], xlab="PC1", ylab="PC2", xlim=c(-270,500))
text(pca$x[,1], pca$x[,2], colnames(x))
```



























